{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Plot Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "**Given a text corpus of movie plots data, create a model that generates new (artificial) plots and deploy it.**\n",
    "\n",
    "The source dataset for this project was taken from [here](<http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz>). We will look into scraping Wikipedia to create a new dataset in a later post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Reading Data\n",
    "\n",
    "First, we read in the data and combine all plots into a single corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he d\n"
     ]
    }
   ],
   "source": [
    "# preparing the corpus\n",
    "# data = pd.read_csv(os.path.join('data', 'input', 'MovieSummaries', 'plot_summaries.txt'), \n",
    "#                    sep='\\t', names=['id', 'plot'])\n",
    "# data = data[:500]\n",
    "# data = list(data['plot'])\n",
    "# corpus = ' '.join(data)\n",
    "with open(os.path.join('data', 'input', 'Seinfeld_Scripts.txt'), 'r') as file:\n",
    "    corpus = file.read()\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    def __init__(self, corpus):\n",
    "        self.word2count = {}\n",
    "        self.word2index = {}\n",
    "        self.index2word = {0: '<PADDING>'}\n",
    "        self.num_words = 1 # <PADDING>\n",
    "        self.min_count = 3\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        # call create_vocab on object creation\n",
    "        self.create_vocab(corpus)\n",
    "        \n",
    "    # replace punctuations\n",
    "    def remove_punctuations(self, text):\n",
    "        for punct in '!\"#$%&().*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r':\n",
    "            text = text.replace(punct, ' ')\n",
    "        return text\n",
    "\n",
    "    # preprocess text and return list of words\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.strip()\n",
    "        text = text.lower()\n",
    "        text = self.remove_punctuations(text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = text.split(' ')\n",
    "        return text\n",
    "    \n",
    "    # add each word into the dictionaries\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1        \n",
    "    \n",
    "    # preprocess data and create a list of words\n",
    "    # then add each word to the dictionaries\n",
    "    def create_vocab(self, text):\n",
    "        # preprocess corpus\n",
    "        self.corpus = self.preprocess_text(text)\n",
    "        \n",
    "        # add words to dictionaries\n",
    "        for word in self.corpus:\n",
    "            self.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of corpus = 605614\n",
      "Number of Unique words in corpus = 19863\n",
      "\n",
      "Sample Corpus: \n",
      "['jerry', 'do', 'you', 'know', 'what', 'this', 'is', 'all', 'about', 'do', 'you', 'know', 'why', 'were', 'here', 'to', 'be', 'out', 'this', 'is', 'out', 'and', 'out', 'is', 'one', 'of', 'the', 'single', 'most', 'enjoyable', 'experiences', 'of', 'life', 'people', 'did', 'you', 'ever', 'hear', 'people', 'talking', 'about', 'we', 'should', 'go', 'out', 'this', 'is', 'what', 'theyre', 'talking']\n"
     ]
    }
   ],
   "source": [
    "# create Vocabulary object\n",
    "vocab = Vocabulary(corpus)\n",
    "\n",
    "print(f'Size of corpus = {len(corpus.split())}')\n",
    "print(f'Number of Unique words in corpus = {len(vocab.word2index)}\\n')\n",
    "\n",
    "print(f'Sample Corpus: \\n{vocab.corpus[:50]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Tokens: \n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 10, 11, 12, 13, 14, 15, 6, 7, 15, 16, 15, 7, 17, 18, 19, 20, 21, 22, 23, 18, 24, 25, 26, 3, 27, 28, 25, 29, 9, 30, 31, 32, 15, 6, 7, 5, 33, 29]\n"
     ]
    }
   ],
   "source": [
    "# convert each word to numerical token using word2index\n",
    "def encode_corpus(vocab):\n",
    "    encoded_corpus = []\n",
    "    for word in vocab.corpus:\n",
    "        encoded_corpus.append(vocab.word2index[word])\n",
    "    return encoded_corpus\n",
    "\n",
    "encoded_tokens = encode_corpus(vocab)\n",
    "print(f'Encoded Tokens: \\n{encoded_tokens[:50]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list of tokens into sequence data\n",
    "# extract features and labels from the sequence data as numpy arrays\n",
    "def create_training_data(encoded_tokens, sequence_length=10, batch_size=50):\n",
    "    # a sequence of length 20 has 19 features and one target\n",
    "    # we need to get full batches containing full sequences\n",
    "    n_tokens = len(encoded_tokens)\n",
    "    n_possible_sequences = n_tokens - sequence_length + 1\n",
    "    n_batches = n_possible_sequences // batch_size\n",
    "    n_sequences = n_batches * batch_size\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    for ii in range(n_sequences):\n",
    "        feature = encoded_tokens[ii : ii+sequence_length-1]\n",
    "        label = encoded_tokens[ii+sequence_length]\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # convert to numpy array\n",
    "    features, labels = np.array(features), np.array(labels)\n",
    "    \n",
    "    # convert features and labels data to Long Tensor\n",
    "    features, labels = torch.from_numpy(features), torch.from_numpy(labels)\n",
    "    features, labels = features.long(), labels.long()\n",
    "    \n",
    "    # convert features and labels data into DataLoader for easy batching\n",
    "    training_data = TensorDataset(features, labels)\n",
    "    training_data = DataLoader(training_data, batch_size=batch_size)\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Tensor: \n",
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [ 2,  3,  4,  5,  6,  7,  8,  9,  2],\n",
      "        [ 3,  4,  5,  6,  7,  8,  9,  2,  3],\n",
      "        [ 4,  5,  6,  7,  8,  9,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9,  2,  3,  4, 10],\n",
      "        [ 6,  7,  8,  9,  2,  3,  4, 10, 11],\n",
      "        [ 7,  8,  9,  2,  3,  4, 10, 11, 12],\n",
      "        [ 8,  9,  2,  3,  4, 10, 11, 12, 13],\n",
      "        [ 9,  2,  3,  4, 10, 11, 12, 13, 14],\n",
      "        [ 2,  3,  4, 10, 11, 12, 13, 14, 15],\n",
      "        [ 3,  4, 10, 11, 12, 13, 14, 15,  6],\n",
      "        [ 4, 10, 11, 12, 13, 14, 15,  6,  7],\n",
      "        [10, 11, 12, 13, 14, 15,  6,  7, 15],\n",
      "        [11, 12, 13, 14, 15,  6,  7, 15, 16],\n",
      "        [12, 13, 14, 15,  6,  7, 15, 16, 15]])\n",
      "\n",
      "Label Tensor: \n",
      "tensor([ 3,  4, 10, 11, 12, 13, 14, 15,  6,  7, 15, 16, 15,  7, 17])\n"
     ]
    }
   ],
   "source": [
    "# get training data\n",
    "input_dataloader = create_training_data(encoded_tokens, sequence_length=10, batch_size=15)\n",
    "\n",
    "batch_x, batch_y = next(iter(input_dataloader))\n",
    "print(f'Feature Tensor: \\n{batch_x}\\n')\n",
    "print(f'Label Tensor: \\n{batch_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed the data preparation phase. Lets now move on to model creation and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=1024, n_layers=2, dropout=0.5):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # set class variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = vocab_size # output_size is the same as vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # define layers\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, self.output_size)\n",
    "        \n",
    "    def forward(self, batch_input):\n",
    "\n",
    "        batch_size = batch_input.size(0)\n",
    "        \n",
    "        embed_out = self.embed(batch_input)\n",
    "        lstm_out, _ = self.lstm(embed_out)\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        # reshape into (batch_size, seq_length, output_size)\n",
    "        fc_out = fc_out.view(batch_size, -1, self.output_size)\n",
    "        \n",
    "        # get last values of each sequence. \n",
    "        # size = (batch_size, output_size) \n",
    "        # the output of each sequence is of size output_size = vocab_size\n",
    "        output = fc_out[:, -1]\n",
    "        \n",
    "        # return one batch of output word scores\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embed): Embedding(19863, 100)\n",
      "  (lstm): LSTM(100, 1024, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=1024, out_features=19863, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model summary\n",
    "model = Net(vocab_size=len(vocab.word2index))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward and backward pass\n",
    "def train(model, epochs, optimizer, loss_funtion, device):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for idx, batch in enumerate(input_dataloader):\n",
    "            batch_x, batch_y = batch\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = loss_funtion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "            \n",
    "            if (idx+1) % 100 == 0:\n",
    "                print(f'-- Epoch: {epoch}/{epochs}, Loss: {total_loss/(idx+1)}')\n",
    "            \n",
    "        print(f'Epoch: {epoch}, Avg. Batch Loss:{total_loss/(idx+1)}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameteres for batching\n",
    "sequence_length = 10\n",
    "batch_size = 128\n",
    "\n",
    "# set parameters for training\n",
    "epochs = 20\n",
    "vocab_size = len(vocab.word2index)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 1024\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get encoded tokens\n",
    "encoded_tokens = encode_corpus(vocab)\n",
    "\n",
    "# get training data\n",
    "input_dataloader = create_training_data(encoded_tokens, sequence_length, batch_size)\n",
    "\n",
    "# instantiate model and send to model\n",
    "model = Net(vocab_size, embedding_dim, hidden_dim, n_layers, dropout=dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch: 1/20, Loss: 7.779703888893128\n",
      "-- Epoch: 1/20, Loss: 7.123715350627899\n",
      "-- Epoch: 1/20, Loss: 6.910825576782226\n",
      "-- Epoch: 1/20, Loss: 6.873457415103912\n",
      "-- Epoch: 1/20, Loss: 6.81625286769867\n",
      "-- Epoch: 1/20, Loss: 6.793596826394399\n",
      "-- Epoch: 1/20, Loss: 6.749405933107648\n",
      "-- Epoch: 1/20, Loss: 6.724829170107841\n",
      "-- Epoch: 1/20, Loss: 6.696787780655755\n",
      "-- Epoch: 1/20, Loss: 6.679587941169739\n",
      "-- Epoch: 1/20, Loss: 6.664028927629644\n",
      "-- Epoch: 1/20, Loss: 6.644272829294205\n",
      "-- Epoch: 1/20, Loss: 6.639522865735567\n",
      "-- Epoch: 1/20, Loss: 6.6302850931031365\n",
      "-- Epoch: 1/20, Loss: 6.624685676892598\n",
      "-- Epoch: 1/20, Loss: 6.624357369244098\n",
      "-- Epoch: 1/20, Loss: 6.620013927571914\n",
      "-- Epoch: 1/20, Loss: 6.614863878356086\n",
      "-- Epoch: 1/20, Loss: 6.6145558841604934\n",
      "-- Epoch: 1/20, Loss: 6.617357450723648\n",
      "-- Epoch: 1/20, Loss: 6.614528046562558\n",
      "-- Epoch: 1/20, Loss: 6.613967206261375\n",
      "-- Epoch: 1/20, Loss: 6.614150090632231\n",
      "-- Epoch: 1/20, Loss: 6.612934514085452\n",
      "-- Epoch: 1/20, Loss: 6.611910195922851\n",
      "-- Epoch: 1/20, Loss: 6.614152316496923\n",
      "-- Epoch: 1/20, Loss: 6.605557363828023\n",
      "-- Epoch: 1/20, Loss: 6.602956658942359\n",
      "-- Epoch: 1/20, Loss: 6.6008869291174\n",
      "-- Epoch: 1/20, Loss: 6.593567786693573\n",
      "-- Epoch: 1/20, Loss: 6.593270479632962\n",
      "-- Epoch: 1/20, Loss: 6.590284566134215\n",
      "-- Epoch: 1/20, Loss: 6.590009958238313\n",
      "-- Epoch: 1/20, Loss: 6.590564011405496\n",
      "-- Epoch: 1/20, Loss: 6.590694340569632\n",
      "-- Epoch: 1/20, Loss: 6.588938908179601\n",
      "-- Epoch: 1/20, Loss: 6.587403360186396\n",
      "-- Epoch: 1/20, Loss: 6.592684437099256\n",
      "-- Epoch: 1/20, Loss: 6.596242546179356\n",
      "-- Epoch: 1/20, Loss: 6.600218830347061\n",
      "-- Epoch: 1/20, Loss: 6.603503264915652\n",
      "-- Epoch: 1/20, Loss: 6.604213086536952\n",
      "-- Epoch: 1/20, Loss: 6.6048241563176\n",
      "-- Epoch: 1/20, Loss: 6.605647927305915\n",
      "-- Epoch: 1/20, Loss: 6.605465735223558\n",
      "-- Epoch: 1/20, Loss: 6.605806923430899\n",
      "-- Epoch: 1/20, Loss: 6.605163303334662\n",
      "Epoch: 1, Avg. Batch Loss:6.607792679007206\n",
      "-- Epoch: 2/20, Loss: 6.113836765289307\n",
      "-- Epoch: 2/20, Loss: 6.0586784553527835\n",
      "-- Epoch: 2/20, Loss: 6.052068854967753\n",
      "-- Epoch: 2/20, Loss: 6.075123933553695\n",
      "-- Epoch: 2/20, Loss: 6.073585485458374\n",
      "-- Epoch: 2/20, Loss: 6.085316271781921\n",
      "-- Epoch: 2/20, Loss: 6.076658701215472\n",
      "-- Epoch: 2/20, Loss: 6.074979946017265\n",
      "-- Epoch: 2/20, Loss: 6.064159231715732\n",
      "-- Epoch: 2/20, Loss: 6.058996772289276\n",
      "-- Epoch: 2/20, Loss: 6.055301850058815\n",
      "-- Epoch: 2/20, Loss: 6.047451430161794\n",
      "-- Epoch: 2/20, Loss: 6.048836534206684\n",
      "-- Epoch: 2/20, Loss: 6.049517553193229\n",
      "-- Epoch: 2/20, Loss: 6.052007936795553\n",
      "-- Epoch: 2/20, Loss: 6.057749163806438\n",
      "-- Epoch: 2/20, Loss: 6.060683214524213\n",
      "-- Epoch: 2/20, Loss: 6.0614623792966205\n",
      "-- Epoch: 2/20, Loss: 6.06660152083949\n",
      "-- Epoch: 2/20, Loss: 6.072471637248993\n",
      "-- Epoch: 2/20, Loss: 6.073273363794599\n",
      "-- Epoch: 2/20, Loss: 6.0760807516358115\n",
      "-- Epoch: 2/20, Loss: 6.078974774609441\n",
      "-- Epoch: 2/20, Loss: 6.081559408307076\n",
      "-- Epoch: 2/20, Loss: 6.0836830884933475\n",
      "-- Epoch: 2/20, Loss: 6.086961781978607\n",
      "-- Epoch: 2/20, Loss: 6.081778005494011\n",
      "-- Epoch: 2/20, Loss: 6.083182502644402\n",
      "-- Epoch: 2/20, Loss: 6.084080625073663\n",
      "-- Epoch: 2/20, Loss: 6.08134245856603\n",
      "-- Epoch: 2/20, Loss: 6.083770172980524\n",
      "-- Epoch: 2/20, Loss: 6.084618284553289\n",
      "-- Epoch: 2/20, Loss: 6.087951272617687\n",
      "-- Epoch: 2/20, Loss: 6.092018519008861\n",
      "-- Epoch: 2/20, Loss: 6.09500208568573\n",
      "-- Epoch: 2/20, Loss: 6.097275103727976\n",
      "-- Epoch: 2/20, Loss: 6.099130487570892\n",
      "-- Epoch: 2/20, Loss: 6.10675450425399\n",
      "-- Epoch: 2/20, Loss: 6.11372118081802\n",
      "-- Epoch: 2/20, Loss: 6.121113332152366\n",
      "-- Epoch: 2/20, Loss: 6.125882696756502\n",
      "-- Epoch: 2/20, Loss: 6.128769459383828\n",
      "-- Epoch: 2/20, Loss: 6.13192480309065\n",
      "-- Epoch: 2/20, Loss: 6.135234677249735\n",
      "-- Epoch: 2/20, Loss: 6.137331015480889\n",
      "-- Epoch: 2/20, Loss: 6.140564707673114\n",
      "-- Epoch: 2/20, Loss: 6.142839581712764\n",
      "Epoch: 2, Avg. Batch Loss:6.146139534434887\n",
      "-- Epoch: 3/20, Loss: 5.918902773857116\n",
      "-- Epoch: 3/20, Loss: 5.871763947010041\n",
      "-- Epoch: 3/20, Loss: 5.857313365936279\n",
      "-- Epoch: 3/20, Loss: 5.8702851581573485\n",
      "-- Epoch: 3/20, Loss: 5.868230768203736\n",
      "-- Epoch: 3/20, Loss: 5.874285275141398\n",
      "-- Epoch: 3/20, Loss: 5.8640107801982335\n",
      "-- Epoch: 3/20, Loss: 5.861622540950775\n",
      "-- Epoch: 3/20, Loss: 5.847985964881049\n",
      "-- Epoch: 3/20, Loss: 5.840599275112152\n",
      "-- Epoch: 3/20, Loss: 5.835744599862532\n",
      "-- Epoch: 3/20, Loss: 5.826543924411138\n",
      "-- Epoch: 3/20, Loss: 5.826360320311326\n",
      "-- Epoch: 3/20, Loss: 5.827373096602304\n",
      "-- Epoch: 3/20, Loss: 5.829966875076294\n",
      "-- Epoch: 3/20, Loss: 5.8349488812685015\n",
      "-- Epoch: 3/20, Loss: 5.837132755167344\n",
      "-- Epoch: 3/20, Loss: 5.8371430219544305\n",
      "-- Epoch: 3/20, Loss: 5.841862254895662\n",
      "-- Epoch: 3/20, Loss: 5.846545622587204\n",
      "-- Epoch: 3/20, Loss: 5.847081416675023\n",
      "-- Epoch: 3/20, Loss: 5.849436442635276\n",
      "-- Epoch: 3/20, Loss: 5.852204004370648\n",
      "-- Epoch: 3/20, Loss: 5.854693579673767\n",
      "-- Epoch: 3/20, Loss: 5.8565200338363645\n",
      "-- Epoch: 3/20, Loss: 5.859227192035088\n",
      "-- Epoch: 3/20, Loss: 5.854461661974589\n",
      "-- Epoch: 3/20, Loss: 5.855417953048433\n",
      "-- Epoch: 3/20, Loss: 5.856697634006369\n",
      "-- Epoch: 3/20, Loss: 5.854306402524313\n",
      "-- Epoch: 3/20, Loss: 5.856174856308968\n",
      "-- Epoch: 3/20, Loss: 5.8568235051631925\n",
      "-- Epoch: 3/20, Loss: 5.85998444037004\n",
      "-- Epoch: 3/20, Loss: 5.864042032466215\n",
      "-- Epoch: 3/20, Loss: 5.866262124606541\n",
      "-- Epoch: 3/20, Loss: 5.868337176375919\n",
      "-- Epoch: 3/20, Loss: 5.870050612011472\n",
      "-- Epoch: 3/20, Loss: 5.876850150384401\n",
      "-- Epoch: 3/20, Loss: 5.883833375832974\n",
      "-- Epoch: 3/20, Loss: 5.891032258987427\n",
      "-- Epoch: 3/20, Loss: 5.895296187517119\n",
      "-- Epoch: 3/20, Loss: 5.897711362271082\n",
      "-- Epoch: 3/20, Loss: 5.900659779171611\n",
      "-- Epoch: 3/20, Loss: 5.903670782717792\n",
      "-- Epoch: 3/20, Loss: 5.905895899984571\n",
      "-- Epoch: 3/20, Loss: 5.9091646806053495\n",
      "-- Epoch: 3/20, Loss: 5.911183180403202\n",
      "Epoch: 3, Avg. Batch Loss:5.913698835405035\n",
      "-- Epoch: 4/20, Loss: 5.737144165039062\n",
      "-- Epoch: 4/20, Loss: 5.704199013710022\n",
      "-- Epoch: 4/20, Loss: 5.689987724622091\n",
      "-- Epoch: 4/20, Loss: 5.69814849972725\n",
      "-- Epoch: 4/20, Loss: 5.698202478408813\n",
      "-- Epoch: 4/20, Loss: 5.700915750662486\n",
      "-- Epoch: 4/20, Loss: 5.691024490765163\n",
      "-- Epoch: 4/20, Loss: 5.689097363948822\n",
      "-- Epoch: 4/20, Loss: 5.674391585456\n",
      "-- Epoch: 4/20, Loss: 5.666878610610962\n",
      "-- Epoch: 4/20, Loss: 5.66181856025349\n",
      "-- Epoch: 4/20, Loss: 5.652830061912536\n",
      "-- Epoch: 4/20, Loss: 5.653147072058458\n",
      "-- Epoch: 4/20, Loss: 5.6549964145251685\n",
      "-- Epoch: 4/20, Loss: 5.658065756479899\n",
      "-- Epoch: 4/20, Loss: 5.662749451100826\n",
      "-- Epoch: 4/20, Loss: 5.665278081893921\n",
      "-- Epoch: 4/20, Loss: 5.6659195658895705\n",
      "-- Epoch: 4/20, Loss: 5.670350791780572\n",
      "-- Epoch: 4/20, Loss: 5.67501913523674\n",
      "-- Epoch: 4/20, Loss: 5.675807413146609\n",
      "-- Epoch: 4/20, Loss: 5.677720267772674\n",
      "-- Epoch: 4/20, Loss: 5.680502641512\n",
      "-- Epoch: 4/20, Loss: 5.68266913274924\n",
      "-- Epoch: 4/20, Loss: 5.6841546148300175\n",
      "-- Epoch: 4/20, Loss: 5.686415418478159\n",
      "-- Epoch: 4/20, Loss: 5.682040213302329\n",
      "-- Epoch: 4/20, Loss: 5.6827928904124665\n",
      "-- Epoch: 4/20, Loss: 5.683935842349611\n",
      "-- Epoch: 4/20, Loss: 5.681348406473796\n",
      "-- Epoch: 4/20, Loss: 5.682758952879136\n",
      "-- Epoch: 4/20, Loss: 5.6832158564031126\n",
      "-- Epoch: 4/20, Loss: 5.686316729747888\n",
      "-- Epoch: 4/20, Loss: 5.690113836877487\n",
      "-- Epoch: 4/20, Loss: 5.691780346325466\n",
      "-- Epoch: 4/20, Loss: 5.693481801218456\n",
      "-- Epoch: 4/20, Loss: 5.695037574510317\n",
      "-- Epoch: 4/20, Loss: 5.7011498743609375\n",
      "-- Epoch: 4/20, Loss: 5.707730526801868\n",
      "-- Epoch: 4/20, Loss: 5.7145647748708726\n",
      "-- Epoch: 4/20, Loss: 5.718308617894243\n",
      "-- Epoch: 4/20, Loss: 5.7204746507463\n",
      "-- Epoch: 4/20, Loss: 5.723188181921493\n",
      "-- Epoch: 4/20, Loss: 5.725865066159855\n",
      "-- Epoch: 4/20, Loss: 5.728146848572625\n",
      "-- Epoch: 4/20, Loss: 5.731319043429002\n",
      "-- Epoch: 4/20, Loss: 5.733046176788655\n",
      "Epoch: 4, Avg. Batch Loss:5.734945120133133\n",
      "-- Epoch: 5/20, Loss: 5.581910853385925\n",
      "-- Epoch: 5/20, Loss: 5.558128294944763\n",
      "-- Epoch: 5/20, Loss: 5.543830324808757\n",
      "-- Epoch: 5/20, Loss: 5.549173454046249\n",
      "-- Epoch: 5/20, Loss: 5.549395300865173\n",
      "-- Epoch: 5/20, Loss: 5.549518156846364\n",
      "-- Epoch: 5/20, Loss: 5.5390501955577305\n",
      "-- Epoch: 5/20, Loss: 5.536347470879555\n",
      "-- Epoch: 5/20, Loss: 5.5219701899422535\n",
      "-- Epoch: 5/20, Loss: 5.514801157951355\n",
      "-- Epoch: 5/20, Loss: 5.509625014825301\n",
      "-- Epoch: 5/20, Loss: 5.50142774105072\n",
      "-- Epoch: 5/20, Loss: 5.502005037894616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch: 5/20, Loss: 5.504362803527287\n",
      "-- Epoch: 5/20, Loss: 5.5075583788553875\n",
      "-- Epoch: 5/20, Loss: 5.511598843634129\n",
      "-- Epoch: 5/20, Loss: 5.514033405079561\n",
      "-- Epoch: 5/20, Loss: 5.515080654621125\n",
      "-- Epoch: 5/20, Loss: 5.519378108727304\n",
      "-- Epoch: 5/20, Loss: 5.52396782875061\n",
      "-- Epoch: 5/20, Loss: 5.524882966450282\n",
      "-- Epoch: 5/20, Loss: 5.526963618235155\n",
      "-- Epoch: 5/20, Loss: 5.529897459693577\n",
      "-- Epoch: 5/20, Loss: 5.531869138479233\n",
      "-- Epoch: 5/20, Loss: 5.532787777900696\n",
      "-- Epoch: 5/20, Loss: 5.534234920831827\n",
      "-- Epoch: 5/20, Loss: 5.530354097507618\n",
      "-- Epoch: 5/20, Loss: 5.530872511863708\n",
      "-- Epoch: 5/20, Loss: 5.531938068784516\n",
      "-- Epoch: 5/20, Loss: 5.52896101029714\n",
      "-- Epoch: 5/20, Loss: 5.5297551664229365\n",
      "-- Epoch: 5/20, Loss: 5.529952114373446\n",
      "-- Epoch: 5/20, Loss: 5.53269480344021\n",
      "-- Epoch: 5/20, Loss: 5.536108456499436\n",
      "-- Epoch: 5/20, Loss: 5.537040718759809\n",
      "-- Epoch: 5/20, Loss: 5.538418718443976\n",
      "-- Epoch: 5/20, Loss: 5.5395874547958375\n",
      "-- Epoch: 5/20, Loss: 5.544767344876339\n",
      "-- Epoch: 5/20, Loss: 5.5509331209231645\n",
      "-- Epoch: 5/20, Loss: 5.557127149820328\n",
      "-- Epoch: 5/20, Loss: 5.560329122078128\n",
      "-- Epoch: 5/20, Loss: 5.562270595686776\n",
      "-- Epoch: 5/20, Loss: 5.564648492613504\n",
      "-- Epoch: 5/20, Loss: 5.567140544652939\n",
      "-- Epoch: 5/20, Loss: 5.569502726978726\n",
      "-- Epoch: 5/20, Loss: 5.572463664282923\n",
      "-- Epoch: 5/20, Loss: 5.574007864099868\n",
      "Epoch: 5, Avg. Batch Loss:5.575486635619944\n",
      "-- Epoch: 6/20, Loss: 5.443755040168762\n",
      "-- Epoch: 6/20, Loss: 5.424520320892334\n",
      "-- Epoch: 6/20, Loss: 5.409062789281209\n",
      "-- Epoch: 6/20, Loss: 5.4110349237918856\n",
      "-- Epoch: 6/20, Loss: 5.4098342580795284\n",
      "-- Epoch: 6/20, Loss: 5.408801297346751\n",
      "-- Epoch: 6/20, Loss: 5.398451993806021\n",
      "-- Epoch: 6/20, Loss: 5.395870423913002\n",
      "-- Epoch: 6/20, Loss: 5.382717559602526\n",
      "-- Epoch: 6/20, Loss: 5.375730669021606\n",
      "-- Epoch: 6/20, Loss: 5.370588820630854\n",
      "-- Epoch: 6/20, Loss: 5.363126394351323\n",
      "-- Epoch: 6/20, Loss: 5.363709962184613\n",
      "-- Epoch: 6/20, Loss: 5.3665951756068635\n",
      "-- Epoch: 6/20, Loss: 5.369633508046468\n",
      "-- Epoch: 6/20, Loss: 5.373206543922424\n",
      "-- Epoch: 6/20, Loss: 5.375814328474157\n",
      "-- Epoch: 6/20, Loss: 5.376972402996487\n",
      "-- Epoch: 6/20, Loss: 5.381188030493887\n",
      "-- Epoch: 6/20, Loss: 5.385499254941941\n",
      "-- Epoch: 6/20, Loss: 5.386621655509585\n",
      "-- Epoch: 6/20, Loss: 5.388791254433719\n",
      "-- Epoch: 6/20, Loss: 5.391896022299061\n",
      "-- Epoch: 6/20, Loss: 5.3936638057231905\n",
      "-- Epoch: 6/20, Loss: 5.394375818824768\n",
      "-- Epoch: 6/20, Loss: 5.394991290936103\n",
      "-- Epoch: 6/20, Loss: 5.391129991390087\n",
      "-- Epoch: 6/20, Loss: 5.391329891341073\n",
      "-- Epoch: 6/20, Loss: 5.392064017262952\n",
      "-- Epoch: 6/20, Loss: 5.388779481887817\n",
      "-- Epoch: 6/20, Loss: 5.389000964933826\n",
      "-- Epoch: 6/20, Loss: 5.388841500729322\n",
      "-- Epoch: 6/20, Loss: 5.39110391183333\n",
      "-- Epoch: 6/20, Loss: 5.394120340627783\n",
      "-- Epoch: 6/20, Loss: 5.394658476284572\n",
      "-- Epoch: 6/20, Loss: 5.395520417557822\n",
      "-- Epoch: 6/20, Loss: 5.396296371382636\n",
      "-- Epoch: 6/20, Loss: 5.400778866818077\n",
      "-- Epoch: 6/20, Loss: 5.406287627097888\n",
      "-- Epoch: 6/20, Loss: 5.4117366524934765\n",
      "-- Epoch: 6/20, Loss: 5.414559923846547\n",
      "-- Epoch: 6/20, Loss: 5.416348027501788\n",
      "-- Epoch: 6/20, Loss: 5.418234524061513\n",
      "-- Epoch: 6/20, Loss: 5.420331997112794\n",
      "-- Epoch: 6/20, Loss: 5.422651372273763\n",
      "-- Epoch: 6/20, Loss: 5.425301073426786\n",
      "-- Epoch: 6/20, Loss: 5.426580391538904\n",
      "Epoch: 6, Avg. Batch Loss:5.42756279761141\n",
      "-- Epoch: 7/20, Loss: 5.320414509773254\n",
      "-- Epoch: 7/20, Loss: 5.300413253307343\n",
      "-- Epoch: 7/20, Loss: 5.282894784609477\n",
      "-- Epoch: 7/20, Loss: 5.282819620370865\n",
      "-- Epoch: 7/20, Loss: 5.280036083221436\n",
      "-- Epoch: 7/20, Loss: 5.278254757722219\n",
      "-- Epoch: 7/20, Loss: 5.26763746874673\n",
      "-- Epoch: 7/20, Loss: 5.265101033449173\n",
      "-- Epoch: 7/20, Loss: 5.253581659528944\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c70bfbd8e292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-6fa296475df5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, optimizer, loss_funtion, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model = train(model, epochs, optimizer, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk\n",
    "torch.save({'state_dict': model.state_dict()}, \\\n",
    "           os.path.join('data', 'output', f'trained_model_bs{batch_size}_sq{sequence_length}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prime_word, generate_len=100):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # start with prime word\n",
    "    generated_words = [prime_word]\n",
    "    \n",
    "    # create new sequence with shape (1, sequence_length)\n",
    "    # fill with 0 = <PADDING>\n",
    "    gen_sequence = np.full(shape=(1, sequence_length), fill_value=0)\n",
    "    \n",
    "    # replace last value of gen_sequence with encoded prime_word\n",
    "    gen_sequence[-1][-1] = vocab.word2index[prime_word]\n",
    "    \n",
    "    # generate new words generate_len times\n",
    "    for _ in range(generate_len):\n",
    "    \n",
    "        # run the sequence through the model to get next word\n",
    "        gen_sequence = torch.from_numpy(gen_sequence).long()\n",
    "        output = model(gen_sequence)\n",
    "        \n",
    "        # convert raw output values into probabilities\n",
    "        prob = F.softmax(output, dim=1).data\n",
    "        prob = prob.cpu()\n",
    "\n",
    "        # get top_k probable output words and index\n",
    "        top_k = 5\n",
    "        top_k_prob, top_k_idx = prob.topk(top_k)\n",
    "        top_k_prob, top_k_idx = top_k_prob.numpy().squeeze(), top_k_idx.numpy().squeeze()\n",
    "\n",
    "        # get the next words choosing according to probability from top_k words\n",
    "        word_idx = np.random.choice(top_k_idx, p=top_k_prob/np.sum(top_k_prob))\n",
    "\n",
    "        # roll the sequence by 1 in negative direction\n",
    "        # and append the new word to end of sequence\n",
    "        gen_sequence = np.roll(gen_sequence, shift=-1, axis=1)\n",
    "        gen_sequence[-1][-1] = word_idx\n",
    "\n",
    "        # append new word to generated words\n",
    "        generated_words.append(vocab.index2word[word_idx])\n",
    "        \n",
    "    \n",
    "    # convert generated_words list to text\n",
    "    generated_text = ' '.join(generated_words)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "george you what i you to a jerry i i you you i you a you a george a jerry a george jerry a of george yeah a of a jerry a of george jerry what that you me you me the jerry george i i i i i a in i i you you what is about george i know you to her on phone what jerry elaine george you me i you i you what is george i i know i i i i to you i a and jerry know i a elaine i know know is to\n"
     ]
    }
   ],
   "source": [
    "generate_len = 100\n",
    "prime_word = 'george'\n",
    "generated_text = generate_text(model.cpu(), prime_word, generate_len)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text generated here is complete gibberish. To get better results we need to do one or more of the following:\n",
    "\n",
    "- **Larger input data** - We have only taken a small subset of the movie plots dataset. Training on a larger corpus of text will produce better results. But more input data will lead to more unique words which will increase the size of our model. Depending on GPU memory available, very large models may not fit in the GPU.\n",
    "- **More number of epochs** - We trained our model for very few epochs. To get better results, the model need to be trained for more epochs. The downside is that as we increase the number of epochs, our model training time will also increase.\n",
    "- **More complex model** - Our model was very simple with only a few layers. We can increase model complexity to capture more complex relationship between data. But increasing the model complexity will also increase the model size. This will again cause memory issues in GPU.\n",
    "- **Trim rare words** - Our corpus has many words that are only used once or twice in the entire corpus. But each extra word adds mre compleity to the model, making the model bigger and causing memory issues. We can implement a process that remove the rarerely used words along with the sequences that use these rare words. That will improve our model performance significantly.\n",
    "- **Initialize hidden weights** - We have used default weight initialization for our model. We can explicitly define a `init_hidden()` function in the model definition. Better weight initialization can make help the model converge faster.\n",
    "\n",
    "There is one way we can get much better results without facing most of the above issues - **Transfer Learning**. We can use a pre-trained model to hasten the training process and get better results. We will look into Transfer Learning in a future post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
